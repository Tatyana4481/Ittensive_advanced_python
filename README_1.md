Импорт данных
Задание: получение данных по API
Изучите API Геокодера Яндекса
tech.yandex.ru/maps/geocoder/doc/desc/concepts/input_params-docpage/
и получите ключ API для него в кабинете разработчика.
Выполните запрос к API и узнайте долготу точки на карте (Point) для города Самара.
Внимание: активация ключа Геокодера Яндекса может занимать несколько часов (до суток).
В качестве запасного варианта можно использовать этот ключ - 3f355b88-81e9-4bbf-a0a4-eb687fdea256 - он только для выполнения этого задания!

Решение: Подключаем необходимые библиотеки: json, pandas, requests. Делаем запрос, с параметрами согласно API Геокодера Яндекса, формируя поисковую строку. Указываем, что ответ желаем получить в формате json. Разбирая по цепочке полученный ответ формируем соответсвующий запрос.

Data parsing
Задание: получение котировок акций
Получите данные по котировкам акций со страницы:
mfd.ru/marketdata/?id=5&group=16&mode=3&sortHeader=name&sortOrder=1&selectedDate=01.11.2019
и найдите, по какому тикеру был максимальный рост числа сделок (в процентах) за 1 ноября 2019 года.

Решение:
Подключаем библиотеки pandas, requests, BeautifulSoup
Получаем данные со страницы в формате html. Разбираем полученные данные, ориентируясь на id. Создаем пустой список. Находим все тэги <tr> внутри <table>. Из полученной  таблицы проверяем построчно полученный элемент на длину больше 0 и в этом случае добавляем полученный элемент к списку. Сформированный список списков загружаем в DateFrame c указанием названия колонок, удаляя все сделки которые равны N/A. Столбец числа сделок приводим к числовому значению меняя дефис на минус и удаляя знак %. Устанавливаем индекс по столбцу "число сделок в %" , сортируем его от большего значения к меньшему и берем самую первую строку полученного нами DateFrame.

Веб-скрепинг

Задание: парсинг интернет-магазина
Используя парсинг данных с маркетплейса beru.ru, найдите, на сколько литров отличается общий объем холодильников Саратов 263 и Саратов 452?
Для парсинга можно использовать зеркало страницы beru.ru с результатами для холодильников Саратов по адресу:
video.ittensive.com/data/018-python-advanced/beru.ru/

Решение:
Подключаем библиотеки requests, BeautifulSoup
Определяем заголовок, указывая куда в случае какой - либо заинтересованности администраторы интернет - магазина смогут написать.  Составляем запрос, указывая в нем наш заголовок, получаем ответ в формате html.Разбираем ответ, ищем необходимую для нас информацию. Находим все ссылки (используется <a>). Построчно проверяем каждую найденную ссылку и ищем текст "Саратов 263" и "Саратов 452". Затем получаем уже по конкретным полученным  ссылкам. Затем по этим данным ищем объем холодильников (поиск осуществляется с помощью функции). Затем находим разницу в объемах этих холодильников.


Работа с SQL
Задание: загрузка результатов в БД
Соберите данные о моделях холодильников Саратов с маркетплейса beru.ru: URL, название, цена, размеры, общий объем, объем холодильной камеры.
Создайте соответствующие таблицы в SQLite базе данных и загрузите полученные данные в таблицу beru_goods.
Для парсинга можно использовать зеркало страницы beru.ru с результатами для холодильников Саратов по адресу:
video.ittensive.com/data/018-python-advanced/beru.ru/

Решение:
Подключаем библиотеки requests, BeautifulSoup, sqlite3
Создаем функцию find_data с помощью которой будем парсить данные. В ней будем находить данные: заголовок (название холодильника), габариты холодильника (ширина, высота, глубина), общий объем, объем холодильной камеры. Все данные находим по ключевым словам или тегам, которые определили при разборе полученных данных в формате html. Всем параметрам присваиваем 0 для того, если значения нет или оно не может быть приведено к числу, то параметр будет равен 0.Разбирая построчно полученные данные из тэгов находим необходимые значения параметров и сразу переводим их (при необходимости) в числовое значение.
С основной страницы получаем данные по всем холодильникам "Саратов", сразу передаем в функцию find_data и забираем из нее данные списком для дальнейшей работы, если есть название "Саратов" и тег <href> со ссылкой на параметры.
Создаем таблицу SQL, передаем названия полей и их параметры. И ..... можно делать запросы.

Основы Matplotlib

Задание: тип визуализации данных
Загрузите данные по ЕГЭ за последние годы
https://video.ittensive.com/python-advanced/data-9722-2019-10-14.utf.csv
выберите данные за 2018-2019 учебный год.
Выберите тип диаграммы для отображения результатов по административному округу Москвы, постройте выбранную диаграмму для количества школьников, написавших ЕГЭ на 220 баллов и выше.
Выберите тип диаграммы и постройте ее для районов Северо-Западного административного округа Москвы для количества школьников, написавших ЕГЭ на 220 баллов и выше.

Решение:
Подключаем библиотеки pandas, matplotlib
Загружаем данные. преобразуем данные, чтобы избежать дубликатов и длинных названий. Устанавливаем район как категория для ускорения сортировки. Выставляем индекс по году и получаем с помощью фильтрации данные за промежуток 2018-2019 года. После этого сбрасываем индексы. Строим две круговые диаграммы в одну строку.

Визуализация зависимостей
Задание: результаты марафона
Загрузите данные по итогам марафона
https://video.ittensive.com/python-advanced/marathon-data.csv
Приведите время половины и полной дистанции к секундам.
Найдите, данные каких серии данных коррелируют (используя диаграмму pairplot в Seaborn).
Найдите коэффициент корреляции этих серий данных, используя scipy.stats.pearsonr.
Постройте график jointplot для коррелирующих данных

РЕШЕНИЕ:
Подключаем библиотеки pandas, matplotlib, seaborn, scipy.stats
Загружаем данные. Переводим время в секунды, предварительно конвертировав его в числовой вид. Строим парный график по всем парным числовым значениям, чтобы посмотреть какие данные коррелируют друг с другом. Рассчитываем коэффициент Пирсона и строим joint-график.

Временные ряды
Задание: скользящие средние на биржевых графиках
Используя данные индекса РТС за последние годы
https://video.ittensive.com/python-advanced/rts-index.csv
постройте отдельные графики закрытия (Close) индекса по дням за 2017, 2018, 2019 годы в единой оси X.
Добавьте на график экспоненциальное среднее за 20 дней для значения Max за 2017 год.
Найдите последнюю дату, когда экспоненциальное среднее максимального дневного значения (Max) в 2017 году было больше, чем соответствующее значение Close в 2019 году (это последнее пересечение графика за 2019 год и графика для среднего за 2017 год).

Решение:


