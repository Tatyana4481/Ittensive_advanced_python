ИМПОРТ С NUMPY и PANDAS файла *.csv Задание: Возьмите данные по вызовам пожарных служб в Москве за 2015-2019 годы: <<https://video.ittensive.com/python-advanced/> data-5283-2019-10-04.utf.csv> Получите из них фрейм данных (таблицу значений). По этому фрейму вычислите среднее значение вызовов пожарных машин в месяц в одном округе Москвы, округлив до целых Примечание: найдите среднее значение вызовов, без учета года.

Решение: Подключаем библиотеку Pandas. Считываем данные из файла.Заполняем нулями все пропущенные значения. Используя функцию mean рассчитываем среднее значение по столбцу вызовы (Calls), округляем полученное значение до целого числа.

Результат: Округленное среднее значение вызовов пожарных машин за 2015-2019 года в Москве. (493 вызова)

Indexes and combining frames (Индексы и объединение фреймов)

Задание: данные из нескольких источников Получите данные по безработице в Москве: <https://video.ittensive.com/python-advanced/data-9753-2019-07-25.utf.csv> Объедините эти данные индексами (Месяц/Год) с данными из предыдущего задания (вызовы пожарных) для Центральный административный округ: <https://video.ittensive.com/python-advanced/data-5283-2019-10-04.utf.csv> Найдите значение поля UnemployedMen в том месяце, когда было меньше всего вызовов в Центральном административном округе.

Решение: Считываем данные из двух наборов данных. В 1-ом наборе устанавливаем индексы по году и периоды (месяц). Во втором наборе данных также устанавливаем индексы по году и месяцу - это для объединения. И, также еще устанавливаем индекс по Административному округу. Объеденяем оба набора данных. После этого отменяем предыдущие индексы и устанавливаем индекс по колонке "Вызовы", сортируем ее по индексу. После этого выводим строку с индексом 0 (используя срез). И видим, что когда было меньше всего вызовов (220 штук) - безработных мужчин было 13465 человек.

Фильтрация и изменение данных. Задание: Получите данные по безработице в Москве: <https://video.ittensive.com/python-advanced/data-9753-2019-07-25.utf.csv> Найдите, с какого года процент людей с ограниченными возможностями (UnemployedDisabled) среди всех безработных (UnemployedTotal) стал меньше 2%.

Решение: Импортируем наш набор данных с помощью pandas. Чтобы выполнить фильтрациюзаведем дополнительный столбец в DateFrame в котором посчитаем отношение в процентах между столбцами "люли с ограниченными возможностями" (UnemployedDisabled) и "всего безработных" (UnemployedTotal). Установим индекс по колонке "Год". Отфильтруем все значения в этом столбце, которые меньше 2% и отсортируем их. Выведем самый первый индекс(со значением 0). В результате получим искомое значение.

Задание: Предсказание на 2020 год Возьмите данные по безработице в городе Москва: video.ittensive.com/python-advanced/data-9753-2019-07-25.utf.csv Сгруппируйте данные по годам, и, если в году меньше 6 значений, отбросьте эти годы. Постройте модель линейной регрессии по годам среднего значения отношения UnemployedDisabled к UnemployedTotal (процента людей с ограниченными возможностями) за месяц и ответьте, какое ожидается значение процента безработных инвалидов в 2020 году при сохранении текущей политики города Москвы? Ответ округлите до сотых. Например, 2,32

Решение: Подключаем необходимые библиотеки: numpy, pandas, sklearn.linear_model. Загружаем данные сресурса в DateFrame. Создаем дополнительную колонку и для каждой строки вычисляем в процентном соотношении отношение количества людей с ограниченными возможностями к количеству безработных всего. Выполняем группировку по году и отбрасываем те строки (годы), если их количество за год меньше 6. Полученные данные группируем по году и считаем средние значения за каждый год. Создаем модель с помощью model = LinearRegression(). Передаем значения x и y. Для x: Индексы полученных сгруппированных данных приводим к двумерному массиву (используем Numpy) и изменяем размерность массива на двумерный. Для y: Создаем массив по рассчитанной нами и сгруппированной колонке и приводим к двумерному массиву (используем Numpy) и изменяем размерность массива на двумерный. Длина массива определяется длиной данных, сгруппированных по году и рассчитанными средними значениями. Следующим шагом загружаем все данные в модель линейной регрессии. Затем, с помощью функции model.predict делаем предсказание. При выводе информации для пользователя не забываем округлить до 2-х знаков после запятой.

Импорт данных Задание: получение данных по API Изучите API Геокодера Яндекса tech.yandex.ru/maps/geocoder/doc/desc/concepts/input_params-docpage/ и получите ключ API для него в кабинете разработчика. Выполните запрос к API и узнайте долготу точки на карте (Point) для города Самара. Внимание: активация ключа Геокодера Яндекса может занимать несколько часов (до суток). В качестве запасного варианта можно использовать этот ключ - 3f355b88-81e9-4bbf-a0a4-eb687fdea256 - он только для выполнения этого задания!

Решение: Подключаем необходимые библиотеки: json, pandas, requests. Делаем запрос, с параметрами согласно API Геокодера Яндекса, формируя поисковую строку. Указываем, что ответ желаем получить в формате json. Разбирая по цепочке полученный ответ формируем соответсвующий запрос.

Data parsing Задание: получение котировок акций Получите данные по котировкам акций со страницы: mfd.ru/marketdata/?id=5&group=16&mode=3&sortHeader=name&sortOrder=1&selectedDate=01.11.2019 и найдите, по какому тикеру был максимальный рост числа сделок (в процентах) за 1 ноября 2019 года.

Решение: Подключаем библиотеки pandas, requests, BeautifulSoup Получаем данные со страницы в формате html. Разбираем полученные данные, ориентируясь на id. Создаем пустой список. Находим все тэги внутри

. Из полученной таблицы проверяем построчно полученный элемент на длину больше 0 и в этом случае добавляем полученный элемент к списку. Сформированный список списков загружаем в DateFrame c указанием названия колонок, удаляя все сделки которые равны N/A. Столбец числа сделок приводим к числовому значению меняя дефис на минус и удаляя знак %. Устанавливаем индекс по столбцу "число сделок в %" , сортируем его от большего значения к меньшему и берем самую первую строку полученного нами DateFrame.
Веб-скрепинг

Задание: парсинг интернет-магазина Используя парсинг данных с маркетплейса beru.ru, найдите, на сколько литров отличается общий объем холодильников Саратов 263 и Саратов 452? Для парсинга можно использовать зеркало страницы beru.ru с результатами для холодильников Саратов по адресу: video.ittensive.com/data/018-python-advanced/beru.ru/

Решение: Подключаем библиотеки requests, BeautifulSoup Определяем заголовок, указывая куда в случае какой - либо заинтересованности администраторы интернет - магазина смогут написать. Составляем запрос, указывая в нем наш заголовок, получаем ответ в формате html.Разбираем ответ, ищем необходимую для нас информацию. Находим все ссылки (используется ). Построчно проверяем каждую найденную ссылку и ищем текст "Саратов 263" и "Саратов 452". Затем получаем уже по конкретным полученным ссылкам. Затем по этим данным ищем объем холодильников (поиск осуществляется с помощью функции). Затем находим разницу в объемах этих холодильников.

Работа с SQL Задание: загрузка результатов в БД Соберите данные о моделях холодильников Саратов с маркетплейса beru.ru: URL, название, цена, размеры, общий объем, объем холодильной камеры. Создайте соответствующие таблицы в SQLite базе данных и загрузите полученные данные в таблицу beru_goods. Для парсинга можно использовать зеркало страницы beru.ru с результатами для холодильников Саратов по адресу: video.ittensive.com/data/018-python-advanced/beru.ru/

Решение: Подключаем библиотеки requests, BeautifulSoup, sqlite3 Создаем функцию find_data с помощью которой будем парсить данные. В ней будем находить данные: заголовок (название холодильника), габариты холодильника (ширина, высота, глубина), общий объем, объем холодильной камеры. Все данные находим по ключевым словам или тегам, которые определили при разборе полученных данных в формате html. Всем параметрам присваиваем 0 для того, если значения нет или оно не может быть приведено к числу, то параметр будет равен 0.Разбирая построчно полученные данные из тэгов находим необходимые значения параметров и сразу переводим их (при необходимости) в числовое значение. С основной страницы получаем данные по всем холодильникам "Саратов", сразу передаем в функцию find_data и забираем из нее данные списком для дальнейшей работы, если есть название "Саратов" и тег со ссылкой на параметры. Создаем таблицу SQL, передаем названия полей и их параметры. И ..... можно делать запросы.

Основы Matplotlib

Задание: тип визуализации данных Загрузите данные по ЕГЭ за последние годы <https://video.ittensive.com/python-advanced/data-9722-2019-10-14.utf.csv> выберите данные за 2018-2019 учебный год. Выберите тип диаграммы для отображения результатов по административному округу Москвы, постройте выбранную диаграмму для количества школьников, написавших ЕГЭ на 220 баллов и выше. Выберите тип диаграммы и постройте ее для районов Северо-Западного административного округа Москвы для количества школьников, написавших ЕГЭ на 220 баллов и выше.

Решение: Подключаем библиотеки pandas, matplotlib Загружаем данные. преобразуем данные, чтобы избежать дубликатов и длинных названий. Устанавливаем район как категория для ускорения сортировки. Выставляем индекс по году и получаем с помощью фильтрации данные за промежуток 2018-2019 года. После этого сбрасываем индексы. Строим две круговые диаграммы в одну строку.

Визуализация зависимостей Задание: результаты марафона Загрузите данные по итогам марафона <https://video.ittensive.com/python-advanced/marathon-data.csv> Приведите время половины и полной дистанции к секундам. Найдите, данные каких серии данных коррелируют (используя диаграмму pairplot в Seaborn). Найдите коэффициент корреляции этих серий данных, используя scipy.stats.pearsonr. Постройте график jointplot для коррелирующих данных

РЕШЕНИЕ: Подключаем библиотеки pandas, matplotlib, seaborn, scipy.stats Загружаем данные. Переводим время в секунды, предварительно конвертировав его в числовой вид. Строим парный график по всем парным числовым значениям, чтобы посмотреть какие данные коррелируют друг с другом. Рассчитываем коэффициент Пирсона и строим joint-график.

Временные ряды Задание: скользящие средние на биржевых графиках Используя данные индекса РТС за последние годы <https://video.ittensive.com/python-advanced/rts-index.csv> постройте отдельные графики закрытия (Close) индекса по дням за 2017, 2018, 2019 годы в единой оси X. Добавьте на график экспоненциальное среднее за 20 дней для значения Max за 2017 год. Найдите последнюю дату, когда экспоненциальное среднее максимального дневного значения (Max) в 2017 году было больше, чем соответствующее значение Close в 2019 году (это последнее пересечение графика за 2019 год и графика для среднего за 2017 год).

Решение:
Подключаем библиотеки pandas, matplotlib Загружаем данные. Приводим все данные к дням года, заполняем промежутки и строим все на одном графике. Для этого создаем дополнительно колонку "Дата", параллельно переобразуя дату к международному стандарту.Заполняем пустые даты предыдущими значениями. Добавим еще одну колонку с днем года для подписи по оси Х. Установим индекс - колонка "Дата". Отсортируем данные. Создадим две новых серии данных за 2017  и  2019 года и установим в них индексом колонку "День". За 2017 дополнительно сразу рассчитаем экспоненциальное среднее максимального дневного значения. отфильтруем данные за 2019 когда значение колонки "Close" было больше максимума аналогичного дня за 2017 года. Установим индекс по "Дата". Отсортируем по индексу и выведем самое первое значение индекса. Это и будет искомая дата.

Задание: объекты культурного наследия России
Изучите набор данных по объектам культурного наследия России (в виде gz-архива):
<https://video.ittensive.com/python-advanced/data-44-structure-4.csv.gz>
и постройте фоновую картограмму по количеству объектов в каждом регионе России, используя гео-данные
<https://video.ittensive.com/python-advanced/russia.json>
Выведите для каждого региона количество объектов в нем.
Посчитайте число объектов культурного наследия в Татарстане.

Решение: Подключаем библиотеки matplotlib, geopandas, pandas, descartes. Так как файлы с данными имеют большой размер, то загрузим все данные локально и будем их использовать как находящиееся на нашем носителе информации. Для анализа данных будем использовать и загружать в DateFrame только те данные, которые будут нужны для построения карты и подсчета числа объектов.
Подсчитаем количество объектов культурного наследия в каждом регионе. Загрузим геоданные и приведем их сразу к меркатору. Унифицируем названия одного и того же региона указанного в разных DateFrame. Объеденим полученные и исправленные наборы данных. объединение проводим по колонке "Регион".
Создаем холст и область отрисовки. На каждый отрисованный регион наносим  в центре количество объектов культурного наследия. Так как карта сильно растягивается обрезаем на ней два региона - Калининградскую область и Чукотку.  Итоговой операцией просто выводим для пользователя количество объектов культурного наследия в Татарстане.

Задание: сборка PDF документа
Используя данные по посещаемости библиотек в районах Москвы
<https://video.ittensive.com/python-advanced/data-7361-2019-11-28.utf.json>
постройте круговую диаграмму суммарной посещаемости (NumOfVisitors) 20 наиболее популярных районов Москвы.
Создайте PDF отчет, используя файл
<https://video.ittensive.com/python-advanced/title.pdf>
как первую страницу. На второй странице выведите итоговую диаграмму, самый популярный район Москвы и число посетителей библиотек в нем.

Решение: Подключаем библиотеки: requests, json, pandas, matplotlib, seaborn, а также отдельные библиотеки для работы с PDF-файлами: from reportlab.pdfgen import canvas, from reportlab.lib import pagesizes, from reportlab.pdfbase import pdfmetrics, from reportlab.pdfbase.ttfonts import TTFont, from reportlab.lib.utils import ImageReader,
from PyPDF2 import PdfFileMerger, PdfFileReader.
Загружаем данные в формате json, затем передадим их в DateFrame и заполняем отсутствующие данные нулями.. Определяем, что каждый набор данных это вложенный словарь. Для группировки по району нужно извлечь название района из этой структуры. Используя функцию extract_district извлекаем эти данные. Ищем значение района, находим в нем поле "District", приводим все найденные значения в список и возвращаем первое найденное значение. По факту будет вытащено первое значение "District" из словаря "ObjectAddress". После получения района группируем данные и сортируем их по числу посетителей в порядке убывания.
Строим итоговую круговую диаграмму из 20 самых популярных районов. На самом графике делать надписи не будем - перенесем их в легенду и выведем ее справа от диаграммы. Сохраняем график в файл для дальнейшей вставки в отчет.
Формируем отчет: задаем шрифт, холст с размерами. Нанесем данные по читателям библиотек и укажем, что номер этой страницы 2. Вставляем нашу сохраненную диаграмму из файла. Дополнительно выводим самый популярный район (это первая строка в нашем отсортированном списке) и количество читателей в нем. Объединяем титульную страницу и созданный отчет через PdfFileManager.

Задание: геральдические символы Москвы
Сгенерируйте PDF документ из списка флагов и гербов районов Москвы:
<https://video.ittensive.com/python-advanced/data-102743-2019-11-13.utf.csv>
На каждой странице документа выведите название геральдического символа (Name), его описание (Description) и его изображение (Picture).
Для показа изображений используйте адрес
<https://op.mos.ru/MEDIA/showFile?id=XXX>
где XXX - это значение поля Picture в наборе данных. Например:
<https://op.mos.ru/MEDIA/showFile?id=8466da35-6801-41a9-a71e-04b60408accb>
В случае возникновения проблем с загрузкой изображений с op.mos.ru можно добавить в код настройку для форсирования использования дополнительных видов шифрования в протоколе SSL/TLS.
requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS = 'ALL:@SECLEVEL=1'

Решение: Подключаем библиотеки: pandas, pdfkit.
Формируем html-документ: сформируем заголовок и откроем содержанеи.Добавляем в документ постраничный вывод данных, задаем стиль для всех заголоков, кроме первого, вставляем разрывы страниц. После ввода заголовка задаем стиль для вывода геральдического символа в виде картинки, указываем отступы и источник изображения - добавив к нему номер страницы из нашего корртежа данных. После вывода картинки выводим описание этого геральдического символа, задав ему в стиле размер шрифта.
Сконфигурируем pdf - файл стандартным образом указав: путь куда сохранить, размер страницы, номер страницы. Сгенерируем из строки наш html-файл и сохраним его как heraldic.pdf/

Задание: многостраничный отчет
Используя данные по активностям в парках Москвы
<https://video.ittensive.com/python-advanced/data-107235-2019-12-02.utf.json>
Создайте PDF отчет, в котором выведите:

1. Диаграмму распределения числа активностей по паркам, топ10 самых активных
2. Таблицу активностей по всем паркам в виде Активность-Расписание-Парк

Решение: Подключаем библиотеки: requests, json, pandas, matplotlib, из io import BytesIO, binascii, pdfkit.
Загрузим и сформируем  DateFrame только из нужных нам колонок. Получим значение поля value из словаря NameOfPark и занесем это значение в колонку NameOfPark. Переименуем колонки для лучшего восприятия данных в отчете.
Формируем диаграммуактивности по паркам. Создаем холст. Группируем данные по паркам, отсортируем их по убыванию по колонке "Активность". Чтобы не сохранять локально файлы с изображениями будем использовать библиотеку binascii для хранения изображения в памяти в формате UTF-8. Чтобы pandas не ограничивал длину данных зададим настройки через set_options библиотеки pandas. Через html строку создаем параметры вывода, затем генерируем pdf-отчет, используя данные из html-строки.

Задание: автоматические отчеты
Соберите отчет по результатам ЕГЭ в 2018-2019 году, используя данные
<https://video.ittensive.com/python-advanced/data-9722-2019-10-14.utf.csv>
и отправьте его в HTML формате по адресу support@ittensive.com, используя только Python.
В отчете должно быть:
общее число отличников (учеников, получивших более 220 баллов по ЕГЭ в Москве),
распределение отличников по округам Москвы,
название школы с лучшими результатами по ЕГЭ в Москве.
Диаграмма распределения должна быть вставлена в HTML через data:URI формат (в base64-кодировке).
Дополнительно: приложите к отчету PDF документ того же содержания (дублирующий письмо).

Решение: pandas, matplotlib, pdfkit, из io import BytesIO, binascii, smtplib, из email import encoders, из email.mime.text import MIMEText, из email.mime.base import MIMEBase, из email.mime.multipart import MIMEMultipart
Загружаем данные и выделяем результат за 2018-2019 года. Отсортируем данные, чтобы найти лучшие школы по результатам ЕГЭ. Сгруппируем данные по административным округа, при этом убирая в названии округа все слова кроме первого, чтобы подписи на графиках были более читабельны.
Подсчитаем общее количество отличников - оно будет нужно для дальнейших расчетов.
Создаем холст на котором создадим список секторов. Создаем круговую диаграмму по округам с параметрами вывода и легендой. Два округа с самыми маленькими значениямисделаем выносными для лучшей визуализации.
Чтобы сохранить изображение в памяти будем использовать библиотеку binascii для хранения изображения в памяти в формате UTF-8.Параллельно сохраним изображение локально. Чтобы pandas не ограничивал длину данных зададим настройки через set_options библиотеки pandas. Сформируем html - отчет со всеми необходимыми данными. Сконфигурируем pdf- отчет с указанными настройками и сохраним в pdf-файле.
Для отправки письма по электронной почтесоздаем объект letter = MIMEMultipart(). Задаем поля у этого объекта. В тело письма прикрепим html-документ и после этого вложим в письмо наш отчет. Подключаем smpt-server, указав имя пользователя (отправителя) и его пароль. После этого отправляем писсьмо и закрываем smpt-server.
